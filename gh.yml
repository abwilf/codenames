program: train_gpt_gh.py
method: grid
parameters:
  # all shared parameters here
  seed:
    value: 42
  model_name_or_path:
    value: gpt2
  output_dir:
    value: /work/awilf/siqa/results/test_train

  subtests:
    # gh,clm_every2:
    #   dataset_name:
    #     values:
    #     # - /work/awilf/siqa/tasks/socialiqa_dev.jsonl
    #     - /work/awilf/siqa/tasks/orig_siqa/train.jsonl
    #     # - /work/awilf/siqa/tasks/orig_siqa/combined.jsonl

    #   clm_freeze_enc:
    #     value: 1

    #   run_clm_every:
    #     values: 
    #     - 100
    #     - 200
    #     - 1000
      
    #   clm_max_train_steps:
    #     values: 
    #     - 100
    #     - 200
    #     - 500

    #   N:
    #     value: 1.e+5
      
    #   max_train_steps:
    #     value: 1.e+4
      
    #   eval_every:
    #     value: 2.e+2

    #   n:
    #     values:
    #     - 1
    #     # - 2
    #     # - 3
    #     # - 4
      
    #   p:
    #     values:
    #     - 2
    #     # - 3
    #     # - 4
    #     # - 5
      
    gh,np:
      dataset_name:
        value: /work/awilf/siqa/tasks/orig_siqa/train.jsonl

      clm_freeze_enc:
        value: 1

      learning_rate:
        values:
        - 2.e-5
        - 3.e-5
        - 5.e-5
      
      run_clm_every:
        values:
        - 100000
        # - 200
        # - 1000
      
      # clm_max_train_steps:
      #   values: 
      #   - 100
      #   - 200
      #   - 500

      N:
        values: 
        - 1.e+2
        - 1.e+3
        - 1.e+4
        - 1.e+5
      
      max_train_steps:
        value: 1.e+4
      
      eval_every:
        value: 1.e+2

      np:
        values:
        - 2
        - 3
        - 4
        - 5
      